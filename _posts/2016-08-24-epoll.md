---
layout: blog
categories: unix
title: epoll总结
tags: unix I/O多路复用 epoll
excerpt: epoll总结
---

# I/O多路复用

unix中，当从一个描述符读，然后又写到另一个描述符时，可以在下列形式的循环中使用阻塞I/O：

```c
while ((n = read(STDIN_FILENO, buf, BUFSIZ)) > 0) {
    if (write(STDOUT_FILENO, buf, n) != n)
        err_sys("write_error");
}
```

阻塞I/O能方便的解决这样的问题。但如果必须从两个描述符读，又将如何呢？在这种情况下，我们不能在任何一个描述符上进行阻塞读，否则可能因为该描述符的阻塞导致另外一个描述符即使有数据也无法处理。我们来看下telnet命令的结构，改程序从终端（标准输入）读，将所得数据写到网络连接上，同时从网络连接读，将所得数据写到终端上（标准输出）。在网络连接的另一端，telnetd守护进程读用户键入的命令，并将所读到的送给shell，这如同用户登录到远程机器上一样。telnetd守护进程将执行用户键入命令而产生的输出通过telnet命令送回给用户，并显示在用户终端上。

<img src="/assets/img/unix/telnet.png" width="50%" height="50%">

telnet进程有两个输入，两个输出。我们不能对两个输入中的任意一个使用阻塞read，因为我们不知道到底哪一个输入会得到数据。一个比较好的解决方法是使用**I/O多路复用（I/O multiplexing）**。为了使用这项技术，首先需要构造一张（通常都不止一张）描述符列表，然后调用一个函数，直到这些函数中的一个已准备好进行I/O时，该函数才返回。

# select、poll、epoll

常见的I/O多路复用函数包括select、poll、epoll。

select本质上是通过设置或者检查存放fd标志位的数据结构进行处理。因此存在以下三个缺点：

1. 单个进程所支持的文件描述符数量太小了，默认是1024
2. 需要维护一个用来存放大量fd的数据结构，每次调用select时，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
3. 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大

poll的实现和select非常相似，只是描述fd集合的方式不同，采用的是pollfd（基于链表）结构而不是select的fd\_set，因此并没有fd数量限制。

epoll是对select和poll的改进，能够避免上述的三个问题。相对于select和poll只提供一个函数，epoll提供了三个函数：epoll\_create、epoll\_clt、epoll\_wait。其中epoll\_create是创建一个epoll句柄；epoll\_ctl是注册要监听的事件类型；epoll\_wait则是等待事件的产生。

对于第一个缺点，select受到内核源码的制约：`#define __FD_SETSIZE 1024`。而epoll没有内核限制，但是也有监听fd数量限制，默认数量比较大而已，并且可以通过sysctl设置。

对于第二个缺点，epoll的解决方案在epoll\_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll\_ctl中指定EPOLL\_CTL\_ADD），会把所有的fd拷贝进内核，而不是在epoll\_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

对于第三个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll\_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调事件，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调事件，而这个回调事件会把就绪的fd加入一个就绪链表）。epoll\_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule\_timeout()实现睡一会，判断一会的效果）。

# epoll 接口

epoll包含三个接口，如下：

```c
#include <sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

* `int epoll_create(int size);`

创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

* `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);`

epoll的事件注册函数，它不同于select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。第一个参数是epoll\_create()的返回值，第二个参数表示动作，用三个宏来表示：

```c
EPOLL_CTL_ADD       // 注册新的fd到epfd中
EPOLL_CTL_MOD       // 修改已经注册的fd的监听事件
EPOLL_CTL_DEL       // 从epfd中删除一个fd
```

第三个参数是需要监听的fd，第四个参数是告诉内核需要监听什么事，`struct epoll_event`结构如下：

```c
typedef union epoll_data {
    void *ptr;
    int fd;
    __uint32_t u32;
    __uint64_t u64;
} epoll_data_t;

struct epoll_event {
    __uint32_t events;  /* Epoll events */
    epoll_data_t data;  /* User data variable */
};
```

其中events可以是以下几个宏的集合：

```c
EPOLLIN             // 表示对应的文件描述符可以读（包括对端SOCKET正常关闭）
EPOLLOUT            // 表示对应的文件描述符可以写
EPOLLPRI            // 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）
EPOLLERR            // 表示对应的文件描述符发生错误
EPOLLHUP            // 表示对应的文件描述符被挂断
EPOLLET             // 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的
EPOLLONESHOT        // 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
```

* `int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);`

等待事件的产生，类似于select()调用。参数events表示从内核得到事件的集合，maxevents告诉内核这个events有多大，这个maxevents的值必须大于0，参数timeout是超时时间（毫秒，0表示无论是否有事件到达都立即返回，-1会无限等待）。该函数返回需要处理的事件数目，如返回0表示已超时。

* epoll工作模式

LT(level triggered)是epoll缺省的工作方式，并且同时支持block和no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表。

ET(edge-triggered)是高速工作方式，只支持no-block socket，它效率要比LT更高。ET与LT的区别在于，当一个新的事件到来时，ET模式下当然可以从epoll\_wait调用中获取到这个事件，可是如果这次没有把这个事件对应的套接字缓冲区处理完，在这个套接字中没有新的事件再次到来时，在ET模式下是无法再次从epoll\_wait调用中获取这个事件的。而LT模式正好相反，只要一个事件对应的套接字缓冲区还有数据，就总能从epoll\_wait中获取这个事件。

* epoll框架

如下是一个常用的epoll框架，仅供参考。

```c
struct epoll_event ev, events[maxn];

for(;;)  
{  
    nfds = epoll_wait(epfd, events, 20, 500);  
    for(i = 0; i < nfds; ++i)  
    {  
        if (events[i].data.fd == listenfd)  // 有新的连接  
        {  
            connfd = accept(listenfd, (sockaddr *)&clientaddr, &clilen);    // accept这个连接  
            ev.data.fd = connfd;  
            ev.events = EPOLLIN|EPOLLET;  
            epoll_ctl(epfd, EPOLL_CTL_ADD, connfd, &ev);    // 将新的fd添加到epoll的监听队列中  
        }  
        else if (events[i].events&EPOLLIN)                  // 接收到数据，读socket  
        {  
            n = read(sockfd, line, MAXLINE))                // 读  
            ev.data.ptr = md;                               // md为自定义类型，添加数据  
            ev.events = EPOLLOUT|EPOLLET;  
            epoll_ctl(epfd, EPOLL_CTL_MOD, sockfd, &ev);    // 修改标识符，等待下一个循环时发送数据，异步处理的精髓  
        }  
        else if (events[i].events&EPOLLOUT) // 有数据待发送，写socket  
        {  
            struct myepoll_data* md = (myepoll_data*)events[i].data.ptr;    // 取数据  
            sockfd = md->fd;  
            send(sockfd, md->ptr, strlen((char*)md->ptr), 0);               // 发送数据  
            ev.data.fd = sockfd;  
            ev.events = EPOLLIN|EPOLLET;  
            epoll_ctl(epfd, EPOLL_CTL_MOD, sockfd, &ev);    // 修改标识符，等待下一个循环时接收数据  
        }  
        else  
        {  
            // 其他的处理  
        }  
    }  
}  
```

参考：

[select、poll、epoll之间的区别总结[整理]](http://www.cnblogs.com/Anker/p/3265058.html)
[[linux学习]epoll详解](http://blog.csdn.net/xiajun07061225/article/details/9250579)

